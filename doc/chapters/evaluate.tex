\section{Evaluation}
\label{section-evaluation}

In this section we evaluate our ideas about the debugger that we designed, by answering RQ3. 
The goal of the experiment is to measure the \textit{time} required to solve programming problems~\cite{ko2015practical}. We use time instead of correctness as it matches debugging better: a developer needs to continue debugging \textit{until} he finds an explanation or a solution to his problem. During debugging incorrect assumptions can be tested and corrected. We measure the time from the moment the participant receives the question until the correct answer is given. Participants use either the built-in Chrome Browser debugger (group `Console') or - the treatment - our RxFiddle debugger (group `RxFiddle'). This single alternative debugger together with the experiment UI (which acts as a small IDE) offers all the debugging capabilities subjects of our preliminary interview (RQ1) reported to use.

The experiment consists of a questionnaire, a warm-up task and 4 programming tasks, all available in a single in-browser application. The questionnaire contains questions regarding age, experience in several programming languages and several reactive programming frameworks. We use this self estimation as a measurement of skill instead of a pretest, since it is a faster and better estimator~\cite{kleinschmager2011rate,feigenspan2012measuring,siegmund2014measuring}. The warm-up program is situated in the same environment as the programming problems and contains several tasks designed to let the participants use every control of the test environment. The first 2 programming problems require the participants to obtain an understanding about the behavior of the program and report the findings. The last 2 programming problems contain a program with a bug. The participants are asked to find the event that lead to the bug in the third problem and to identify and propose a solution in the fourth problem. The first 2 problems are synthetic examples of two simple data flows, while the latter contain some mocked (otherwise remote) service which behaves like a real world example.

We use a between-subjects design for our setup. While this complicates the results - subjects have different experience and skills - we can not use a within-subjects design as it would be impossible to control for the learning effect incurred when asking subjects to perform survey questions with and without the tool. This also allows us to restrict the amount of tasks to incorporate in the experiment, requiring less time of our busy subjects.

%\todo{Discuss guidelines defined in ko2015practical~\cite{ko2015practical}}

\subsection{Context}
The experiment was run in a controlled and an online setting.
The controlled experiment was conducted at a Dutch software engineering company. Subjects are developers with several years of programming experience, and range from little to no experience with RP to many years of experience (Figure \ref{fig-experience}). Some of the subjects had already used RxFiddle, forming a potential threat to validity. As we do not try to measure the effect of learning a new tool, but rather using a tool after learning to use it, we explained RxFiddle in the introductory talk and added the warm-up question to get every participant to a minimum amount of knowledge about the debugger at hand.

The online experiment was announced by several core contributors to RP libraries on Twitter and via various other communication channels. Subjects to the online experiment took the test at their own preferred location and have possibly very different backgrounds. Several short video tutorials were created and included in the online experiment to introduce the participants to the debug tool available to them and the tasks they needed to fulfill.

\begin{figure}[h]
\includegraphics[width=\columnwidth]{images/experience.pdf}
\caption{Experience in various programming languages, 9-point Likert scale}
\label{fig-experience}
\end{figure}

\input{chapters/results}